[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastkaggle",
    "section": "",
    "text": "Either:\npip install fastkaggle\nor:\nmamba install -c fastai fastkaggle\n(or replace mamba with conda if you don’t mind it taking much longer to run…)"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "fastkaggle",
    "section": "How to use",
    "text": "How to use\n\nCompetition\nThis little library is where I’ll be putting snippets of stuff which are useful on Kaggle. Functionality includes the following:\nIt defines iskaggle which is True if you’re running on Kaggle:\n\n'Kaggle' if iskaggle else 'Not Kaggle'\n\n'Not Kaggle'\n\n\nIt provides a setup_comp function which gets a path to the data for a competition, downloading it if needed, and also installs any modules that might be missing or out of data if running on Kaggle:\n\nsetup_comp('titanic')\n\nPath('titanic')\n\n\nThere’s also push_notebook to push a notebook to Kaggle Notebooks, and import_kaggle to use the Kaggle API (even when you’re on Kaggle!) See the fastkaggle.core docs for details.\n\n\nDatasets\nThis section is designed to make uploading pip libraries to kaggle datasets easy. There’s 2 primary high level functions to be used. First we can define our kaggle username and the local path we want to use to store datasets when we create them.\n\n\n\n\n\n\nUsage tip\n\n\n\nThe purpose of this is to create datasets that can be used in no internet inference competitions to install libraries using pip install -Uqq library --no-index --find-links=file:///kaggle/input/your_dataset/\n\n\n\nlib_path = Path.home()/'kaggle_datasets'\nusername = 'isaacflath'\n\n\nList of Libraries\nWe can take a list of libraries and upload them as seperate datasets. For example the below will create a library-fastcore and library-timm dataset. If they already exist, it will push a new version if there is a more recent version available.\n\nlibs = ['fastcore','flask','fastkaggle']\ncreate_libs_datasets(libs,lib_path,username)\n\nProcessing fastcore as library-fastcore at /Users/isaacflath/kaggle_datasets/library-fastcore\n-----Downloading or Creating Dataset\n-----Checking dataset version against pip\n-----Kaggle dataset already up to date 1.5.16 to 1.5.16\nProcessing flask as library-flask at /Users/isaacflath/kaggle_datasets/library-flask\n-----Downloading or Creating Dataset\n-----Checking dataset version against pip\n-----Kaggle dataset already up to date 2.2.2 to 2.2.2\nProcessing fastkaggle as library-fastkaggle at /Users/isaacflath/kaggle_datasets/library-fastkaggle\n-----Downloading or Creating Dataset\n-----Checking dataset version against pip\n-----Kaggle dataset already up to date 0.0.6 to 0.0.6\nComplete\n\n\nThis creats datasets in kaggle with the needed files. For example the library fastkaggle looks like this in kaggle.\n\n\n\nFastkaggle Dataset\n\n\n\n\nrequirements.txt\nWe can also create a singular dataset with multiple libraries based on a requirements.txt file for the project. If there are any different files it will push a new version.\n\ncreate_requirements_dataset('test_files/requirements.txt',lib_path,'libraries-pawpularity', username)\n\nProcessing libraries-pawpularity at /root/kaggle_datasets/libraries-pawpularity\n-----Downloading or Creating Dataset\nData package template written to: /root/kaggle_datasets/libraries-pawpularity/dataset-metadata.json\n-----Checking dataset version against pip\n-----Updating libraries-pawpularity in Kaggle\nComplete\n\n\nThis creates a dataset in kaggle with the needed files.\n\n\n\nPawpularity Dataset"
  },
  {
    "objectID": "core.html#datasets",
    "href": "core.html#datasets",
    "title": "fastkaggle.core",
    "section": "Datasets",
    "text": "Datasets\n\nCore\n\n\ncheck_ds_exists\n\n check_ds_exists (dataset_slug)\n\nChecks if a dataset exists in kaggle and returns boolean\n\n\n\n\nDetails\n\n\n\n\ndataset_slug\nDataset slug (ie “zillow/zecon”)\n\n\n\n\n\n\nmk_dataset\n\n mk_dataset (dataset_path, title, force=False, upload=True)\n\nCreates minimal dataset metadata needed to push new dataset to kaggle\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\nLocal path to create dataset in\n\n\ntitle\n\n\nName of the dataset\n\n\nforce\nbool\nFalse\nShould it overwrite or error if exists?\n\n\nupload\nbool\nTrue\nShould it upload and create on kaggle\n\n\n\n\nmk_dataset('./testds','mytestds',force=True)\nmd = json.load(open('./testds/dataset-metadata.json'))\nassert md['title'] == 'mytestds'\nassert md['id'].endswith('/mytestds')\n\nData package template written to: testds/dataset-metadata.json\n\n\n\n\n\nget_dataset\n\n get_dataset (dataset_path, dataset_slug, unzip=True, force=False)\n\nDownloads an existing dataset and metadata from kaggle\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\nLocal path to download dataset to\n\n\ndataset_slug\n\n\nDataset slug (ie “zillow/zecon”)\n\n\nunzip\nbool\nTrue\nShould it unzip after downloading?\n\n\nforce\nbool\nFalse\nShould it overwrite or error if dataset_path exists?\n\n\n\n\n\n\nget_pip_library\n\n get_pip_library (dataset_path, pip_library, pip_cmd='pip')\n\nDownload the whl files for pip_library and store in dataset_path\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\nLocal path to download pip library to\n\n\npip_library\n\n\nname of library for pip to install\n\n\npip_cmd\nstr\npip\npip base to use (ie “pip3” or “pip”)\n\n\n\n\n\n\nget_pip_libraries\n\n get_pip_libraries (dataset_path, requirements_path, pip_cmd='pip')\n\nDownload whl files for a requirements.txt file and store in dataset_path\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\nLocal path to download pip library to\n\n\nrequirements_path\n\n\npath to requirements file\n\n\npip_cmd\nstr\npip\npip base to use (ie “pip3” or “pip”)\n\n\n\n\ndl_path = Path('./mylib')\nget_pip_library(dl_path,'fastkaggle')\nassert 1==len([o for o in dl_path.ls() if str(o).startswith(f\"{dl_path}/fastkaggle\")])\n\n\n\n\npush_dataset\n\n push_dataset (dataset_path, version_comment)\n\nPush dataset update to kaggle. Dataset path must contain dataset metadata file\n\n\n\n\nDetails\n\n\n\n\ndataset_path\nLocal path where dataset is stored\n\n\nversion_comment\nComment associated with this dataset update\n\n\n\n\n\n\nget_local_ds_ver\n\n get_local_ds_ver (lib_path, lib)\n\nchecks a local copy of kaggle dataset for library version number\n\n\n\n\nDetails\n\n\n\n\nlib_path\nLocal path dataset is stored in\n\n\nlib\nName of library (ie “fastcore”)\n\n\n\n\n\n\nHigh Level\n\n\ncreate_libs_datasets\n\n create_libs_datasets (libs, lib_path, username, clear_after=False)\n\nFor each library, create or update a kaggle dataset with the latest version\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlibs\n\n\nlibrary or list of libraries to create datasets for (ie ’fastcore or [‘fastcore’,‘fastkaggle’]\n\n\nlib_path\n\n\nLocal path to dl/create dataset\n\n\nusername\n\n\nYou username\n\n\nclear_after\nbool\nFalse\nDelete local copies after sync with kaggle?\n\n\n\n\n\n\ncreate_requirements_dataset\n\n create_requirements_dataset (req_fpath, lib_path, title, username,\n                              retain=['dataset-metadata.json'],\n                              version_notes='NewUpdate')\n\nDownload everything needed in a requirements.txt file to a dataset and upload to kaggle\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreq_fpath\n\n\nPath to requirements.txt file\n\n\nlib_path\n\n\nLocal path to dl/create dataset\n\n\ntitle\n\n\nTitle you want the kaggle dataset named\n\n\nusername\n\n\nyou username\n\n\nretain\nlist\n[‘dataset-metadata.json’]\nFiles that should not be removed\n\n\nversion_notes\nstr\nNew Update"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "fastkaggle",
    "section": "",
    "text": "Datasets functionality + Docs (#9), thanks to @Isaac-Flath\n\n2 high level functions allow to either pass a list of libraries or a requirements.txt to maintain and update their own libraries as kaggle datasets.\n\n\n\n\n\n\n\n\n\nfix comp should be competition in setup_comp” (#3), thanks to @n-e-w\n\n\n\n\n\n\n\n\napi not exported (#1)\n\n\n\n\n\n\ninit release"
  }
]