{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastkaggle.core\n",
    "\n",
    "> API details for fastkaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os,json,subprocess, shutil\n",
    "import re\n",
    "from fastcore.utils import *\n",
    "# from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def import_kaggle():\n",
    "    \"Import kaggle API, using Kaggle secrets `kaggle_username` and `kaggle_key` if needed\"\n",
    "    if iskaggle:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        sec = UserSecretsClient()\n",
    "        os.environ['KAGGLE_USERNAME'] = sec.get_secret(\"kaggle_username\")\n",
    "        if not os.environ['KAGGLE_USERNAME']: raise Exception(\"Please insert your Kaggle username and key into Kaggle secrets\")\n",
    "        os.environ['KAGGLE_KEY'] = sec.get_secret(\"kaggle_key\")\n",
    "    from kaggle import api\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#20) [contradictory-my-dear-watson,gan-getting-started,store-sales-time-series-forecasting,tpu-getting-started,digit-recognizer,titanic,house-prices-advanced-regression-techniques,connectx,nlp-getting-started,spaceship-titanic...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = import_kaggle()\n",
    "L(api.competitions_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def setup_comp(competition, install=''):\n",
    "    \"Get a path to data for `competition`, downloading it if needed\"\n",
    "    if iskaggle:\n",
    "        if install:\n",
    "            os.system(f'pip install -Uqq {install}')\n",
    "        return Path('../input')/competition\n",
    "    else:\n",
    "        path = Path(competition)\n",
    "        api = import_kaggle()\n",
    "        if not path.exists():\n",
    "            import zipfile\n",
    "            api.competition_download_cli(str(competition))\n",
    "            zipfile.ZipFile(f'{competition}.zip').extractall(str(competition))\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('titanic')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_comp('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass a list of space separated modules to `install`, they'll be installed if running on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def nb_meta(user, id, title, file, competition=None, private=True, gpu=False, internet=True):\n",
    "    \"Get the `dict` required for a kernel-metadata.json file\"\n",
    "    d = {\n",
    "      \"id\": f\"{user}/{id}\",\n",
    "      \"title\": title,\n",
    "      \"code_file\": file,\n",
    "      \"language\": \"python\",\n",
    "      \"kernel_type\": \"notebook\",\n",
    "      \"is_private\": private,\n",
    "      \"enable_gpu\": gpu,\n",
    "      \"enable_internet\": internet,\n",
    "      \"keywords\": [],\n",
    "      \"dataset_sources\": [],\n",
    "      \"kernel_sources\": []\n",
    "    }\n",
    "    if competition: d[\"competition_sources\"] = [f\"competitions/{competition}\"]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'jhoward/my-notebook',\n",
       " 'title': 'My notebook',\n",
       " 'code_file': 'my-notebook.ipynb',\n",
       " 'language': 'python',\n",
       " 'kernel_type': 'notebook',\n",
       " 'is_private': True,\n",
       " 'enable_gpu': False,\n",
       " 'enable_internet': True,\n",
       " 'keywords': [],\n",
       " 'dataset_sources': [],\n",
       " 'kernel_sources': [],\n",
       " 'competition_sources': ['competitions/paddy-disease-classification']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_meta('jhoward', 'my-notebook', 'My notebook', 'my-notebook.ipynb', competition='paddy-disease-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def push_notebook(user, id, title, file, path='.', competition=None, private=True, gpu=False, internet=True):\n",
    "    \"Push notebook `file` to Kaggle Notebooks\"\n",
    "    meta = nb_meta(user, id, title, file=file, competition=competition, private=private, gpu=gpu, internet=internet)\n",
    "    path = Path(path)\n",
    "    nm = 'kernel-metadata.json'\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    with open(path/nm, 'w') as f: json.dump(meta, f, indent=2)\n",
    "    api = import_kaggle()\n",
    "    api.kernels_push_cli(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Kaggle recommends that the `id` match the *slug* for the title -- i.e it should be the same as the title, but lowercase, no punctuation, and spaces replaced with dashes. E.g:\n",
    "\n",
    "```python\n",
    "push_notebook('jhoward', 'first-steps-road-to-the-top-part-1',\n",
    "              title='First Steps: Road to the Top, Part 1',\n",
    "              file='first-steps-road-to-the-top-part-1.ipynb',\n",
    "              competition='paddy-disease-classification',\n",
    "              private=False, gpu=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_ds_exists(dataset_slug # Dataset slug (ie \"zillow/zecon\")\n",
    "                   ):\n",
    "    '''Checks if a dataset exists in kaggle and returns boolean'''\n",
    "    api = import_kaggle()\n",
    "    ds_search = L(api.dataset_list(mine=True)).filter(lambda x: str(x)==dataset_slug)\n",
    "    if len(ds_search)==1: return True\n",
    "    elif len(ds_search)==0: return False\n",
    "    else: raise exception(\"Multiple datasets found - Check Manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_dataset(dataset_path, # Local path to create dataset in\n",
    "               title, # Name of the dataset\n",
    "               force=False, # Should it overwrite or error if exists?\n",
    "               upload=True # Should it upload and create on kaggle\n",
    "              ):\n",
    "    '''Creates minimal dataset metadata needed to push new dataset to kaggle'''\n",
    "    dataset_path = Path(dataset_path)\n",
    "    dataset_path.mkdir(exist_ok=force,parents=True)\n",
    "    api = import_kaggle()\n",
    "    api.dataset_initialize(dataset_path)\n",
    "    md = json.load(open(dataset_path/'dataset-metadata.json'))\n",
    "    md['title'] = title\n",
    "    md['id'] = md['id'].replace('INSERT_SLUG_HERE',title)\n",
    "    json.dump(md,open(dataset_path/'dataset-metadata.json','w'))\n",
    "    if upload: (dataset_path/'empty.txt').touch()\n",
    "    api.dataset_create_new(str(dataset_path),public=True,dir_mode='zip',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data package template written to: testds/dataset-metadata.json\n"
     ]
    }
   ],
   "source": [
    "mk_dataset('./testds','mytestds',force=True)\n",
    "md = json.load(open('./testds/dataset-metadata.json'))\n",
    "assert md['title'] == 'mytestds'\n",
    "assert md['id'].endswith('/mytestds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dataset(dataset_path, # Local path to download dataset to\n",
    "                dataset_slug, # Dataset slug (ie \"zillow/zecon\")\n",
    "                unzip=True, # Should it unzip after downloading?\n",
    "                force=False # Should it overwrite or error if dataset_path exists?\n",
    "               ):\n",
    "    '''Downloads an existing dataset and metadata from kaggle'''\n",
    "    if not force: assert not Path(dataset_path).exists()\n",
    "    api = import_kaggle()\n",
    "    api.dataset_metadata(dataset_slug,str(dataset_path))\n",
    "    api.dataset_download_files(dataset_slug,str(dataset_path))\n",
    "    if unzip:\n",
    "        zipped_file = Path(dataset_path)/f\"{dataset_slug.split('/')[-1]}.zip\"\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(zipped_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(Path(dataset_path))\n",
    "        zipped_file.unlink()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pip_library(dataset_path, # Local path to download pip library to\n",
    "                    pip_library, # name of library for pip to install\n",
    "                    pip_cmd=\"pip\" # pip base to use (ie \"pip3\" or \"pip\")\n",
    "                   ):    \n",
    "    '''Download the whl files for pip_library and store in dataset_path'''\n",
    "    bashCommand = f\"{pip_cmd} download {pip_library} -d {dataset_path}\"\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pip_libraries(dataset_path, # Local path to download pip library to\n",
    "                    requirements_path, # path to requirements file\n",
    "                      pip_cmd=\"pip\" # pip base to use (ie \"pip3\" or \"pip\")\n",
    "                     ):\n",
    "    '''Download whl files for a requirements.txt file and store in dataset_path'''\n",
    "    bashCommand = f\"{pip_cmd} download -r {requirements_path} -d {dataset_path}\"\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_path = Path('./mylib')\n",
    "get_pip_library(dl_path,'fastkaggle')\n",
    "assert 1==len([o for o in dl_path.ls() if str(o).startswith(f\"{dl_path}/fastkaggle\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def push_dataset(dataset_path, # Local path where dataset is stored \n",
    "                 version_comment # Comment associated with this dataset update\n",
    "                ):\n",
    "    '''Push dataset update to kaggle.  Dataset path must contain dataset metadata file'''\n",
    "    api = import_kaggle()\n",
    "    api.dataset_create_version(str(dataset_path),version_comment,dir_mode='zip',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_local_ds_ver(lib_path, # Local path dataset is stored in\n",
    "                     lib # Name of library (ie \"fastcore\")\n",
    "                    ):\n",
    "    '''checks a local copy of kaggle dataset for library version number'''\n",
    "    wheel_lib_name = lib.replace('-','_')\n",
    "    lib_whl = (lib_path/f\"library-{lib}\").ls().filter(lambda x: wheel_lib_name in x.name.lower())\n",
    "    if 1==len(lib_whl):\n",
    "        return re.search(f\"(?<={wheel_lib_name}-)[\\d+.]+\\d\",lib_whl[0].name.lower())[0]\n",
    "    else: return \"No Version Found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_libs_datasets(libs, # library or list of libraries to create datasets for (ie 'fastcore or ['fastcore','fastkaggle']\n",
    "                         lib_path, # Local path to dl/create dataset\n",
    "                         username, # You username\n",
    "                         clear_after=False # Delete local copies after sync with kaggle?\n",
    "                        ):\n",
    "    '''For each library, create or update a kaggle dataset with the latest version'''\n",
    "    if type(libs)==str: libs = [libs] \n",
    "    \n",
    "    retain = [\"dataset-metadata.json\"]\n",
    "    for lib in libs:\n",
    "        title = f\"library-{lib}\"\n",
    "        local_path = lib_path/title\n",
    "        print(f\"{lib} | Processing as {title} at {local_path}\")\n",
    "        if Path(local_path).exists(): shutil.rmtree(local_path)\n",
    "\n",
    "        print(f\"{lib} | Downloading or Creating Dataset\")\n",
    "        try: get_dataset(local_path,f\"{username}/{title}\",force=True)\n",
    "        except Exception as ex:\n",
    "            if '404' in str(ex): mk_dataset(local_path,title,force=True)\n",
    "            else: raise ex\n",
    "            \n",
    "        print(f\"{lib} | Checking dataset version against pip\")\n",
    "        ver_local_orig = get_local_ds_ver(lib_path,lib)\n",
    "\n",
    "        for item in local_path.ls():\n",
    "            if item.name not in retain: \n",
    "                if item.is_dir(): shutil.rmtree(item)\n",
    "                else: item.unlink()\n",
    "        get_pip_library(local_path,lib)\n",
    "        \n",
    "        ver_local_new = get_local_ds_ver(lib_path,lib)\n",
    "\n",
    "        if ver_local_new != ver_local_orig: \n",
    "            print(f\"{lib} | Updating {lib} in Kaggle from {ver_local_orig} to {ver_local_new}\")\n",
    "            push_dataset(local_path,ver_local_new)\n",
    "        else: print(f\"{lib} | Kaggle dataset already up to date {ver_local_orig} to {ver_local_new}\")\n",
    "        if clear_after: shutil.rmtree(local_path)\n",
    "        print(f\"{lib} | Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_requirements_dataset(req_fpath, # Path to requirements.txt file\n",
    "                                lib_path,#Local path to dl/create dataset\n",
    "                                title, # Title you want the kaggle dataset named\n",
    "                                username, # you username\n",
    "                                retain = [\"dataset-metadata.json\"], # Files that should not be removed\n",
    "                                version_notes = \"New Update\"\n",
    "                               ):\n",
    "    '''Download everything needed in a `requirements.txt` file to a dataset and upload to kaggle'''\n",
    "    local_path = lib_path/title\n",
    "    print(f\"Processing {title} at {local_path}\")\n",
    "    if Path(local_path).exists(): shutil.rmtree(local_path)\n",
    "\n",
    "    print(f\"-----Downloading or Creating Dataset\")\n",
    "    if check_ds_exists(f\"{username}/{title}\"): \n",
    "        get_dataset(local_path,f\"{username}/{title}\",force=True)\n",
    "    else:                                       \n",
    "        mk_dataset(local_path,title,force=True)\n",
    "\n",
    "    print(f\"-----Checking dataset version against pip\")\n",
    "    orig_ds = Path(local_path).ls().sorted()\n",
    "    for item in local_path.ls():\n",
    "        if item.name not in retain: \n",
    "            if item.is_dir(): shutil.rmtree(item)\n",
    "            else: item.unlink()\n",
    "    get_pip_libraries(local_path,req_fpath) \n",
    "    \n",
    "    new_ds = Path(local_path).ls().sorted()\n",
    "    \n",
    "    if orig_ds != new_ds: \n",
    "        print(f\"-----Updating {title} in Kaggle\")\n",
    "        push_dataset(local_path,version_notes)\n",
    "    else: print(f\"-----Kaggle dataset already up to date\")\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
